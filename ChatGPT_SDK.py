import os
import time
from openai import OpenAI, OpenAIError
from typing import Optional, List, Dict, Union, Tuple

class ChatGPTClient:
    """
    ä¸€ä¸ªç®€æ´çš„ ChatGPT SDKï¼Œç”¨äºä¸ OpenAI API è¿›è¡Œäº¤äº’ã€‚

    ä¸»è¦åŠŸèƒ½:
    - åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ã€‚
    - å‘é€èŠå¤©è¯·æ±‚å¹¶è·å–å›å¤ã€‚
    - æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ã€ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯å†å²ã€‚
    - ç»Ÿè®¡ token ä½¿ç”¨é‡å’Œè°ƒç”¨æ—¶é—´ã€‚

    ä½¿ç”¨æ–¹æ³•:
    1. è®¾ç½®ç¯å¢ƒå˜é‡ `OPENAI_API_KEY` ä¸ºä½ çš„ API å¯†é’¥ã€‚
    2. åˆ›å»º `ChatGPTClient` å®ä¾‹ã€‚
    3. è°ƒç”¨ `chat` æ–¹æ³•è¿›è¡Œå¯¹è¯ã€‚

    ç¤ºä¾‹:
        >>> client = ChatGPTClient()
        >>> response = client.chat("ä½ å¥½ï¼ŒChatGPTï¼")
        >>> print(response)
        ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ

        >>> history = [
        ...     {"role": "user", "content": "ç ç©†æœ—ç›å³°æœ‰å¤šé«˜ï¼Ÿ"},
        ...     {"role": "assistant", "content": "ç ç©†æœ—ç›å³°çš„æœ€æ–°æµ‹é‡é«˜åº¦æ˜¯8848.86ç±³ã€‚"}
        ... ]
        >>> response = client.chat("è°æµ‹é‡äº†è¿™ä¸ªé«˜åº¦ï¼Ÿ", conversation_history=history)
        >>> print(response)
        è¿™ä¸ªé«˜åº¦æ˜¯ç”±ä¸­å›½å’Œå°¼æ³Šå°”ä¸¤å›½çš„æµ‹é‡ç™»å±±é˜Ÿåœ¨2020å¹´å…±åŒæµ‹é‡çš„ã€‚
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4.1-mini"):
        """
        åˆå§‹åŒ– ChatGPTClientã€‚

        å‚æ•°:
            api_key (Optional[str]): OpenAI API å¯†é’¥ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡ `OPENAI_API_KEY` è¯»å–ã€‚
            model (str): é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "gpt-4o", "gpt-3.5-turbo"ã€‚
        """
        if api_key is None:
            api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("APIå¯†é’¥æœªæä¾›ã€‚è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥ api_keyå‚æ•°ã€‚")

        self.client = OpenAI(api_key=api_key)
        self.default_model = model
        self.total_tokens_used = 0
        self.total_calls = 0

    def chat(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 40000,  # æ›´åˆç†çš„é»˜è®¤å€¼
        **kwargs  # å…è®¸ä¼ é€’å…¶ä»– OpenAI API å‚æ•°
    ) -> Tuple[Optional[str], Dict]:
        """
        å‘ ChatGPT å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ã€‚

        å‚æ•°:
            prompt (str): ç”¨æˆ·å½“å‰çš„è¾“å…¥ã€‚
            system_message (Optional[str]): ç³»ç»Ÿæ¶ˆæ¯ï¼Œç”¨äºè®¾å®šåŠ©æ‰‹çš„è¡Œä¸ºæˆ–è§’è‰²ã€‚
            conversation_history (Optional[List[Dict[str, str]]]):
                ä¹‹å‰çš„å¯¹è¯å†å²ã€‚åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­—å…¸åº”åŒ…å« "role" ("user" æˆ– "assistant") å’Œ "content"ã€‚
            model (Optional[str]): è¦ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„é»˜è®¤æ¨¡å‹ã€‚
            temperature (float): æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œ0 è¡¨ç¤ºæœ€ç¡®å®šæ€§ï¼Œ2 è¡¨ç¤ºæœ€éšæœºã€‚
            max_tokens (int): ç”Ÿæˆå“åº”çš„æœ€å¤§ token æ•°é‡ã€‚
            **kwargs: å…¶ä»–å¯ä»¥ä¼ é€’ç»™ OpenAI API `chat.completions.create` æ–¹æ³•çš„å‚æ•°ã€‚

        è¿”å›:
            Tuple[Optional[str], Dict]: åŒ…å« ChatGPT çš„å›å¤æ–‡æœ¬å’Œä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯çš„å…ƒç»„ã€‚
            å¦‚æœå‘ç”Ÿé”™è¯¯åˆ™å›å¤æ–‡æœ¬ä¸º Noneã€‚ç»Ÿè®¡ä¿¡æ¯åŒ…å« tokens_used, elapsed_time ç­‰ã€‚
        """
        messages = []
        used_model = model if model else self.default_model
        
        # æ˜¾ç¤ºè°ƒç”¨å‰çš„æé†’
        print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨æ¨¡å‹: {used_model}...")
        
        if system_message:
            messages.append({"role": "system", "content": system_message})

        if conversation_history:
            messages.extend(conversation_history)

        messages.append({"role": "user", "content": prompt})

        stats = {
            "model": used_model,
            "tokens_used": 0,
            "elapsed_time": 0,
            "success": False
        }
        
        start_time = time.time()
        
        try:
            completion = self.client.chat.completions.create(
                model=used_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # è®¡ç®—ä½¿ç”¨çš„æ—¶é—´
            elapsed_time = time.time() - start_time
            
            # è·å–ä½¿ç”¨çš„ token æ•°é‡
            prompt_tokens = completion.usage.prompt_tokens
            completion_tokens = completion.usage.completion_tokens
            total_tokens = completion.usage.total_tokens
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.total_tokens_used += total_tokens
            self.total_calls += 1
            
            stats.update({
                "success": True,
                "elapsed_time": elapsed_time,
                "tokens_used": total_tokens,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens
            })
            
            response_content = completion.choices[0].message.content
            result = response_content.strip() if response_content else None
            
            # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
            print(f"âœ… è°ƒç”¨å®Œæˆ! ç”¨æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š Tokenç»Ÿè®¡: æç¤ºè¯={prompt_tokens}, å›å¤={completion_tokens}, æ€»è®¡={total_tokens}")
            
            return result, stats
            
        except OpenAIError as e:
            elapsed_time = time.time() - start_time
            stats["elapsed_time"] = elapsed_time
            print(f"âŒ å‘ç”Ÿ OpenAI API é”™è¯¯: {e}")
            print(f"â±ï¸ ç”¨æ—¶: {elapsed_time:.2f}ç§’")
            return None, stats
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            stats["elapsed_time"] = elapsed_time
            print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            print(f"â±ï¸ ç”¨æ—¶: {elapsed_time:.2f}ç§’")
            return None, stats
    
    def get_usage_stats(self) -> Dict:
        """è·å–æ€»ä½“ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "total_tokens": self.total_tokens_used,
            "total_calls": self.total_calls,
            "average_tokens_per_call": self.total_tokens_used / self.total_calls if self.total_calls > 0 else 0
        }

# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # ç¡®ä¿ä½ å·²ç»è®¾ç½®äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡
    # ä¾‹å¦‚: export OPENAI_API_KEY='sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'

    print("æ­£åœ¨åˆå§‹åŒ– ChatGPT å®¢æˆ·ç«¯...")
    try:
        # 1. åŸºæœ¬ç”¨æ³•
        print("\n--- ç¤ºä¾‹ 1: åŸºæœ¬å¯¹è¯ ---")
        gpt_client = ChatGPTClient()  # é»˜è®¤ä½¿ç”¨ gpt-4.1-mini
        # ä½ ä¹Ÿå¯ä»¥æŒ‡å®šæ¨¡å‹ï¼Œä¾‹å¦‚: gpt_client = ChatGPTClient(model="gpt-3.5-turbo")

        user_prompt = "ä½ å¥½ï¼ŒChatGPTï¼ä½ èƒ½åšä»€ä¹ˆï¼Ÿ"
        print(f"ç”¨æˆ·: {user_prompt}")
        response, stats = gpt_client.chat(user_prompt)
        if response:
            print(f"ChatGPT: {response}")
        else:
            print("æœªèƒ½è·å–å›å¤ã€‚")

        # 2. å¸¦æœ‰ç³»ç»Ÿæ¶ˆæ¯çš„å¯¹è¯
        print("\n--- ç¤ºä¾‹ 2: å¸¦ç³»ç»Ÿæ¶ˆæ¯çš„å¯¹è¯ ---")
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”æœ‰å…³å®‡å®™çš„é—®é¢˜ï¼Œå¹¶ä¸”è¯­è¨€é£æ ¼è¦åƒèå£«æ¯”äºšã€‚"
        user_prompt_cosmic = "å‘Šè¯‰æˆ‘å…³äºé»‘æ´çš„å¥‡å¦™ä¹‹å¤„ã€‚"
        print(f"ç³»ç»Ÿè®¾å®š: {system_prompt}")
        print(f"ç”¨æˆ·: {user_prompt_cosmic}")
        response_cosmic, stats = gpt_client.chat(user_prompt_cosmic, system_message=system_prompt)
        if response_cosmic:
            print(f"ChatGPT (èå£«æ¯”äºšé£æ ¼): {response_cosmic}")

        # 3. å¸¦æœ‰å¯¹è¯å†å²çš„è¿ç»­å¯¹è¯
        print("\n--- ç¤ºä¾‹ 3: è¿ç»­å¯¹è¯ ---")
        history = []

        user_prompt_1 = "æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"
        print(f"ç”¨æˆ·: {user_prompt_1}")
        response_1, stats = gpt_client.chat(user_prompt_1)
        if response_1:
            print(f"ChatGPT: {response_1}")
            history.append({"role": "user", "content": user_prompt_1})
            history.append({"role": "assistant", "content": response_1})
        else:
            print("æœªèƒ½è·å–å›å¤ã€‚")

        if response_1:  # åªæœ‰åœ¨ç¬¬ä¸€æ¬¡æˆåŠŸåæ‰ç»§ç»­
            user_prompt_2 = "é‚£é‡Œæœ‰ä»€ä¹ˆè‘—åçš„å»ºç­‘ç‰©ï¼Ÿ"
            print(f"ç”¨æˆ·: {user_prompt_2}")
            response_2, stats = gpt_client.chat(user_prompt_2, conversation_history=history)
            if response_2:
                print(f"ChatGPT: {response_2}")
            else:
                print("æœªèƒ½è·å–å›å¤ã€‚")

        # 4. æ›´æ”¹æ¨¡å‹å’Œå‚æ•°
        print("\n--- ç¤ºä¾‹ 4: ä½¿ç”¨ä¸åŒæ¨¡å‹å’Œå‚æ•° ---")
        # æ³¨æ„ï¼šgpt-3.5-turbo å¯èƒ½æ›´å¿«ï¼Œæˆæœ¬æ›´ä½ï¼Œä½†èƒ½åŠ›ä¸å¦‚ gpt-4o
        user_prompt_creative = "å†™ä¸€å¥å…³äºæ˜¥å¤©çš„å°è¯—ã€‚"
        print(f"ç”¨æˆ·: {user_prompt_creative}")
        response_creative, stats = gpt_client.chat(
            user_prompt_creative,
            model="gpt-3.5-turbo",  # ä¸´æ—¶è¦†ç›–é»˜è®¤æ¨¡å‹
            temperature=0.9,
            max_tokens=50
        )
        if response_creative:
            print(f"ChatGPT (gpt-3.5-turbo, T=0.9): {response_creative}")
        
        # æ˜¾ç¤ºæ€»ä½“ä½¿ç”¨ç»Ÿè®¡
        usage = gpt_client.get_usage_stats()
        print("\n--- æ€»ä½“ä½¿ç”¨ç»Ÿè®¡ ---")
        print(f"æ€»è°ƒç”¨æ¬¡æ•°: {usage['total_calls']}")
        print(f"æ€»Tokenæ¶ˆè€—: {usage['total_tokens']}")
        print(f"å¹³å‡æ¯æ¬¡è°ƒç”¨Token: {usage['average_tokens_per_call']:.1f}")

    except ValueError as ve:
        print(f"åˆå§‹åŒ–é”™è¯¯: {ve}")
    except Exception as e:
        print(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")